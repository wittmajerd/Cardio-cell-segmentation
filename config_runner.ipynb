{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wittd\\anaconda3\\Lib\\site-packages\\pytools\\persistent_dict.py:63: RecommendedHashNotFoundWarning: Unable to import recommended hash 'siphash24.siphash13', falling back to 'hashlib.sha256'. Run 'python3 -m pip install siphash24' to install the recommended hash.\n",
      "  warn(\"Unable to import recommended hash 'siphash24.siphash13', \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cupy implementation is not available. Make sure you have the right version of Cupy and CUDA installed.\n",
      "Optional dependecy Dask_image is not installed. Implementations using it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wittd\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from src.dataset_new import create_datasets_new, create_folds\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_all_metrics\n",
    "from src.utils import *\n",
    "\n",
    "from src.models import UNet, UNet2, UNet4, UNet8, UNet2Shallow, UNet4Shallow, UNet8Shallow\n",
    "from src.model_parts import SingleConv, DoubleConv, TripleConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is tiling and overlap rate okay if the input and mask scale is different\n",
    "def test_tiling(config):\n",
    "    if config['tiling']:\n",
    "        tiling_rate = config['tiling_ratio']\n",
    "        overlap_rate = config['overlap_rate']\n",
    "        mask_size = 80 * config['mask_scale']\n",
    "        input_size = 80 * config['input_scale']\n",
    "        bio = torch.zeros((config['biosensor_length'], input_size, input_size))\n",
    "        mask = torch.zeros((mask_size, mask_size))\n",
    "        bio_tiles, mask_tiles = create_tiles(bio, mask, tiling_rate, overlap_rate)\n",
    "        print(f\"Tile test\\nBio tiles: {bio_tiles.shape}, Mask tiles: {mask_tiles.shape}\\n\")\n",
    "        assert bio_tiles.shape[0] == mask_tiles.shape[0], \"Tile number mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(config):\n",
    "    root_path = os.path.join(config['save_path'], config['run_name'])\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "\n",
    "    save_config = config.copy()\n",
    "    save_config['mask_type'] = config['mask_type'].__name__\n",
    "    save_config['model'] = config['model'].__name__\n",
    "    save_config['down_conv'] = config['down_conv'].__name__\n",
    "    save_config['up_conv'] = config['up_conv'].__name__\n",
    "    json.dump(save_config, open(os.path.join(root_path, 'config.json'), 'w'), indent=4)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device {device}')\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    folds, test_dataset = create_folds(config)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    print(\"Datasets created!\")\n",
    "\n",
    "    for i in range(config['k']):\n",
    "        print(f'Starting run {i+1}...')\n",
    "        run_path = os.path.join(root_path, f'run_{i+1}')\n",
    "        os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "        plot_path = os.path.join(run_path, 'plots')\n",
    "        os.makedirs(plot_path, exist_ok=True)\n",
    "        model_checkpoint_path = os.path.join(run_path, 'model_checkpoints')\n",
    "        os.makedirs(model_checkpoint_path, exist_ok=True)\n",
    "\n",
    "        mean, std, train_dataset, val_dataset = folds[i]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        model = config['model'](n_channels=config['biosensor_length'], n_classes=1, \n",
    "                                down_conv=config['down_conv'], up_conv=config['up_conv'], \n",
    "                                mean=mean, std=std, bilinear=config['bilinear'])\n",
    "        print(model.__class__.__name__)\n",
    "\n",
    "        if i == 0:\n",
    "            # Save model summary only once\n",
    "            model_summary = summary(model, depth=5)\n",
    "            with open(os.path.join(root_path, 'model_summary.txt'), 'w') as f:\n",
    "                f.write(str(model_summary))\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        try:\n",
    "            log = train_model(\n",
    "                model,\n",
    "                device,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                config,\n",
    "                amp=True,\n",
    "                checkpoint_dir=model_checkpoint_path,\n",
    "                wandb_dir=root_path,\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            print('Detected OutOfMemoryError!')\n",
    "\n",
    "        # Save training log\n",
    "        json.dump(log, open(os.path.join(run_path, 'log.json'), 'w'), indent=4)\n",
    "        print()\n",
    "        best_epoch = find_best_epoch(log)\n",
    "        print(f'\\nBest epoch: {best_epoch}')\n",
    "        best_checkpoint = torch.load(os.path.join(model_checkpoint_path, f'checkpoint_epoch{best_epoch}.pth'))\n",
    "        lr = best_checkpoint.pop('learning_rate')\n",
    "        model.load_state_dict(best_checkpoint)\n",
    "        model = model.to(device)\n",
    "\n",
    "        best_model_path = os.path.join(run_path, 'best_model.pth')\n",
    "        torch.jit.script(model).save(best_model_path)\n",
    "\n",
    "        test_metrics = evaluate_all_metrics(model, test_loader, device, config['mask_scale'])\n",
    "        print('Test metrics:', json.dumps(test_metrics, indent=4))\n",
    "        json.dump(test_metrics, open(os.path.join(root_path, f'test_result_run{i+1}.json'), 'w'), indent=4)\n",
    "\n",
    "        # plot test results\n",
    "        if config['tiling']:\n",
    "            plot_results_tiles(test_loader, model, device, plot_path, 'Test image', config)\n",
    "        else:\n",
    "            plot_results_image(test_loader, model, device, plot_path, 'Test image', config)\n",
    "\n",
    "        del train_loader, val_loader\n",
    "        del train_dataset, val_dataset\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f'Run {i+1} finished!')\n",
    "\n",
    "    del test_loader, test_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print('All runs finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path': 'C:/7_felev/data',\n",
    "    'save_path': 'C:/7_felev/szakdoga',\n",
    "    # Each run name should be unique, even if the same model is used \n",
    "    # or it will overwrite the previous run\n",
    "    'run_name': 'UNet_len1_doubleconv',\n",
    "    'project_name': 'UNet',\n",
    "    'wandb_logging': True,\n",
    "\n",
    "    # Unet, UNet2, UNet4, UNet8, Unet2Shallow, UNet4Shallow, UNet8Shallow\n",
    "    'model': UNet,\n",
    "    'down_conv': DoubleConv,\n",
    "    'up_conv': DoubleConv,\n",
    "    'bilinear': False,\n",
    "\n",
    "    'biosensor_length': 1,\n",
    "    'input_scale': 1,\n",
    "    # for the imput scaling\n",
    "    'SRRF_mode': 'eSRRF', # 'eSRRF', 'SRRF', None\n",
    "    'mask_scale': 1,\n",
    "    \n",
    "    'batch_size': 26,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.01,\n",
    "    # K-fold cross validation\n",
    "    'k': 5,\n",
    "\n",
    "    'mask_type': bool,\n",
    "    'augment': True,\n",
    "    'normalize': True,\n",
    "    'shuffle': True,\n",
    "    \n",
    "    'tiling': False,\n",
    "    'tiling_ratio': 10,\n",
    "    'overlap_rate': 0.25,\n",
    "\n",
    "    'dilation': 0,\n",
    "}\n",
    "\n",
    "# test_tiling(config)\n",
    "# run_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = config.copy()\n",
    "config3['run_name'] = 'UNet4_len8_singleconv'\n",
    "config3['project_name'] = 'UNet4'\n",
    "config3['model'] = UNet4\n",
    "config3['down_conv'] = config3['up_conv'] = SingleConv\n",
    "config3['mask_scale'] = 4\n",
    "config3['biosensor_length'] = 8\n",
    "run_config(config3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = config3.copy()\n",
    "config4['run_name'] = 'UNet4_len8_doubleconv'\n",
    "config4['down_conv'] = config4['up_conv'] = DoubleConv\n",
    "run_config(config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config5 = config3.copy()\n",
    "config5['run_name'] = 'UNet4_len1_singleconv'\n",
    "config5['biosensor_length'] = 1\n",
    "run_config(config5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config6 = config.copy()\n",
    "config6['run_name'] = 'UNet8_len8_singleconv'\n",
    "config6['project_name'] = 'UNet8'\n",
    "config6['model'] = UNet8\n",
    "config6['down_conv'] = config6['up_conv'] = SingleConv\n",
    "config6['mask_scale'] = 8\n",
    "config6['biosensor_length'] = 8\n",
    "run_config(config6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config7 = config6.copy()\n",
    "config7['run_name'] = 'UNet8_len8_doubleconv'\n",
    "config7['down_conv'] = config7['up_conv'] = DoubleConv\n",
    "run_config(config7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
